# File              : supBarlowTwins.py
# Author            : Xilin Jiang <xilinj2@illinois.edu>
# Date              : 30.04.2022
# Last Modified Date: 30.04.2022
# Last Modified By  : Xilin Jiang <xilinj2@illinois.edu>


seed: 2022
__set_seed: !apply:torch.manual_seed [!ref <seed>]
np_rng: !new:numpy.random.RandomState [!ref <seed>]

resume_interrupt: False
resume_task_idx: 0

time_stamp: placeholder
experiment_name: BarlowTwins
# output_folder: !ref results/<experiment_name>/<seed>
output_base: /mnt/data/zhepei/outputs/cssl_sound/vgg_BarlowTwins
output_folder: !ref <output_base>/<time_stamp>_seed_<seed>+<experiment_name>
train_log: !ref <output_folder>/train_log.txt
save_folder: !ref <output_folder>/save

# Number of classes
n_classes: 308
# cont learning setup
task_classes: !apply:utils.prepare_task_classes
  num_classes: !ref <n_classes>
  num_tasks: 5
  seed: !ref <seed>
replay_num_keep: 0

use_mixup: False
mixup_alpha: 0.4
train_duration: 4.0

# Training parameters
number_of_epochs: 50
batch_size: 32

loss_scale: 1 # !ref 1 / (<out_dim> * <out_dim>)
max_lr: 0.03
min_lr: 0.00000001
warmup_epochs: 10
warmup_lr: !ref <batch_size> * 0 / 256
base_lr: !ref <batch_size> * <max_lr> / 256
final_lr: !ref <batch_size> * <min_lr> / 256

# dataset
sample_rate: 16000

data_folder: "/mnt/data/Sound Sets/VGG-Sound"
label_encoder_path: "./dataset/label_encoder_vggsound_ordered.txt"
prepare_split_csv_fn: !name:dataset.prepare_vggsound.prepare_split_vggsound_csv
  root_dir: !ref <data_folder>
  output_dir: !ref <save_folder>
  task_classes: !ref <task_classes>
  train_split: 0.8
  seed: !ref <seed>

train_dataloader_opts:
  batch_size: !ref <batch_size>
  num_workers: 8
  shuffle: True
  drop_last: True

valid_dataloader_opts:
  batch_size: 32
  num_workers: 8


# Experiment params
auto_mix_prec: True # Set it to True for mixed precision

# Feature parameters
n_mels: 80
left_frames: 0
right_frames: 0
deltas: False
amp_to_db: False
normalize: True
win_length: 25
hop_length: 10
n_fft: !ref <win_length> * <sample_rate> // 1000
f_min: 0
use_time_roll: False
use_freq_shift: False
emb_dim: 2048
out_dim: 8192
emb_norm_type: bn
proj_norm_type: bn

# augmentation
# time_domain_aug: !new:speechbrain.lobes.augment.TimeDomainSpecAugment
#     sample_rate: !ref <sample_rate>
#     # drop_chunk_count_high: 2
#     # drop_chunk_noise_factor: 0.05
#     speeds: [90, 95, 100, 105, 110]
#     drop_freq_count_high: 4
#     drop_chunk_count_high: 3
#     # drop_chunk_length_low: 1000
#     # drop_chunk_length_high: 5000
spec_domain_aug: !new:augmentation.TFAugmentation
    time_warp: True
    time_warp_window: 8
    freq_mask: True
    freq_mask_width: !tuple (0, 10)
    n_freq_mask: 2
    time_mask: True
    time_mask_width: !tuple (0, 10)
    n_time_mask: 2
    replace_with_zero: True
    time_roll: !ref <use_time_roll>
    time_roll_limit: !tuple (0, 200)
    freq_shift: !ref <use_freq_shift>
    freq_shift_limit: !tuple (-10, 10)


# Functions
compute_features: !new:speechbrain.lobes.features.Fbank
    n_mels: !ref <n_mels>
    left_frames: !ref <left_frames>
    right_frames: !ref <right_frames>
    deltas: !ref <deltas>
    sample_rate: !ref <sample_rate>
    n_fft: !ref <n_fft>
    win_length: !ref <win_length>
    hop_length: !ref <hop_length>
    f_min: !ref <f_min>

mean_var_norm: !new:speechbrain.processing.features.InputNormalization
    norm_type: sentence
    std_norm: False

# embedding_model: !new:models.tdnn.ECAPA_TDNN
#     input_size: !ref <n_mels>
#     channels: [1024, 1024, 1024, 1024, 3072]
#     kernel_sizes: [5, 3, 3, 3, 1]
#     dilations: [1, 2, 3, 4, 1]
#     attention_channels: 128
#     lin_neurons: !ref <emb_dim>
    # channels: [512, 512, 512, 1024]
    # kernel_sizes: [5, 3, 3, 1]
    # dilations: [1, 2, 3, 1]
    # attention_channels: 128
    # lin_neurons: !ref <emb_dim>
embedding_model: !new:models.pann.Cnn14
    mel_bins: !ref <n_mels>
    emb_dim: !ref <emb_dim>
    norm_type: !ref <emb_norm_type>

projector: !new:models.modules.SimSiamProjector
    input_size: !ref <emb_dim>
    hidden_size: !ref <out_dim>
    output_size: !ref <out_dim>

predictor: !new:models.modules.SimSiamPredictor
    input_size: !ref <out_dim>
    hidden_size: !ref <out_dim>

classifier: !new:models.modules.Classifier
    input_size: !ref <emb_dim>
    output_size: !ref <n_classes>

modules:
    compute_features: !ref <compute_features>
    embedding_model: !ref <embedding_model>
    classifier: !ref <classifier>
    projector: !ref <projector>
    predictor: !ref <predictor>
    mean_var_norm: !ref <mean_var_norm>

ssl_weight: 1.
compute_BarlowTwins_cost: !new:losses.BarlowTwinsLoss
    lambda_rr: 0.005
    out_dim: !ref <out_dim>
    loss_scale: !ref <loss_scale>

sup_weight: 0.
compute_sup_cost: !new:losses.LogSoftmaxWithProbWrapper
    loss_fn: !new:torch.nn.Identity

acc_metric: !name:speechbrain.utils.Accuracy.AccuracyStats

# opt_class: !name:torch.optim.Adam
#     lr: !ref <base_lr>
#     weight_decay: 0.0005
#
# lr_scheduler_fn: !name:speechbrain.nnet.schedulers.CyclicLRScheduler
#     base_lr: !ref <final_lr>
#     max_lr: !ref <base_lr>
#     step_size: 888

opt_class: !name:torch.optim.SGD
    lr: !ref <base_lr>
    weight_decay: 0.0005
    momentum: 0.9

lr_scheduler_fn: !name:schedulers.SimSiamCosineScheduler
    warmup_epochs: !ref <warmup_epochs>
    warmup_lr: !ref <warmup_lr>
    num_epochs: !ref <number_of_epochs>
    base_lr: !ref <base_lr>
    final_lr: !ref <final_lr>
    steps_per_epoch: 200
    constant_predictor_lr: True

epoch_counter_fn: !name:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <number_of_epochs>

datapoint_counter: !new:utils.DatapointCounter
      
# # Logging + checkpoints
checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>

recoverables:
    embedding_model: !ref <embedding_model>
    classifier: !ref <classifier>
    projector: !ref <projector>
    predictor: !ref <predictor>
    normalizer: !ref <mean_var_norm>
    datapoint_counter: !ref <datapoint_counter>

prev_checkpointer: null
# prev_checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
#     checkpoints_dir: !PLACEHOLDER

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <train_log>

# wandb
use_wandb: False
train_log_frequency: 20
wandb_logger_fn: !name:utils.MyWandBLogger
    initializer: !name:wandb.init
    entity: CAL
    project: cssl_sound
    name: !ref <time_stamp>+seed_<seed>+<experiment_name>
    dir: !ref <output_folder>
    reinit: True
    yaml_config: hparams/vgg/supBarlowTwins_train.yaml
    resume: False
