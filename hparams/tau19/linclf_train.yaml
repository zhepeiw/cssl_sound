# File              : linclf_train.yaml
# Author            : Zhepei Wang <zhepeiw2@illinois.edu>
# Date              : 27.01.2022
# Last Modified Date: 31.03.2022
# Last Modified By  : Zhepei Wang <zhepeiw2@illinois.edu>


seed: 2022
__set_seed: !apply:torch.manual_seed [!ref <seed>]
np_rng: !new:numpy.random.RandomState [!ref <seed>]

resume_interrupt: False
resume_task_idx: 0

time_stamp: placeholder
experiment_name: linclf
# output_folder: !ref results/<experiment_name>/<seed>
output_base: /mnt/data/zhepei/outputs/cssl_sound/tdnn_linclf
output_folder: !ref <output_base>/<time_stamp>_seed_<seed>+<experiment_name>
train_log: !ref <output_folder>/train_log.txt
save_folder: !ref <output_folder>/save

# cont learning setup
linclf_train_type: buffer
task_classes:
  # - !tuple (0, 1)
  # - !tuple (2, 3)
  # - !tuple (4, 5)
  # - !tuple (6, 7)
  # - !tuple (8, 9)
  - !tuple (3, 8)
  - !tuple (9, 4)
  - !tuple (1, 7)
  - !tuple (5, 2)
  - !tuple (6, 0)
# task_classes:
#     - !tuple (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)
replay_num_keep: 100

use_mixup: False
mixup_alpha: 0.4

# Training parameters
number_of_epochs: 100
batch_size: 32
# lr: 0.001
# base_lr: 0.00000001
# max_lr: !ref <lr>
# step_size: 65000
warmup_epochs: 0
warmup_lr: !ref <batch_size> * 0 / 256
base_lr: !ref <batch_size> * 0.03 / 256
final_lr: !ref <batch_size> * 0.00000001 / 256

# dataset
sample_rate: 16000
# data_folder: "/mnt/data/Sound Sets/UrbanSound8K/UrbanSound8K"
# train_folds: [1, 2, 3, 4, 5, 6, 7, 8]
# valid_folds: [9]
# test_folds: [10]
# label_encoder_path: "./dataset/label_encoder_us8k_ordered.txt"
# prepare_split_csv_fn: !name:dataset.prepare_urbansound8k.prepare_split_urbansound8k_csv
#   root_dir: !ref <data_folder>
#   output_dir: !ref <save_folder>
#   task_classes: !ref <task_classes>
#   train_folds: !ref <train_folds>
#   valid_folds: !ref <valid_folds>
#   test_folds: !ref <test_folds>

data_folder: "/mnt/data/Sound Sets/TAU-urban-acoustic-scenes-2019-development"
label_encoder_path: "./dataset/label_encoder_tauuas2019_ordered.txt"
prepare_split_csv_fn: !name:dataset.prepare_tauuas2019.prepare_split_tauuas2019_csv
  root_dir: !ref <data_folder>
  output_dir: !ref <save_folder>
  task_classes: !ref <task_classes>

train_dataloader_opts:
  batch_size: !ref <batch_size>
  num_workers: 8
  shuffle: True
  drop_last: True

valid_dataloader_opts:
  batch_size: 32
  num_workers: 8


# Experiment params
auto_mix_prec: True # Set it to True for mixed precision


# Feature parameters
n_mels: 80
left_frames: 0
right_frames: 0
deltas: False
amp_to_db: False
normalize: True
win_length: 25
hop_length: 10
n_fft: !ref <win_length> * <sample_rate> // 1000
f_min: 0
# Number of classes
n_classes: 10
emb_dim: 2048
emb_norm_type: bn

# Functions
compute_features: !new:speechbrain.lobes.features.Fbank
    n_mels: !ref <n_mels>
    left_frames: !ref <left_frames>
    right_frames: !ref <right_frames>
    deltas: !ref <deltas>
    sample_rate: !ref <sample_rate>
    n_fft: !ref <n_fft>
    win_length: !ref <win_length>
    hop_length: !ref <hop_length>
    f_min: !ref <f_min>

mean_var_norm: !new:speechbrain.processing.features.InputNormalization
    norm_type: sentence
    std_norm: False

# embedding_model: !new:models.tdnn.ECAPA_TDNN
#     input_size: !ref <n_mels>
#     channels: [1024, 1024, 1024, 1024, 3072]
#     kernel_sizes: [5, 3, 3, 3, 1]
#     dilations: [1, 2, 3, 4, 1]
#     attention_channels: 128
#     lin_neurons: !ref <emb_dim>
# embedding_model: !new:models.tdnn.ECAPA_TDNN
#     input_size: !ref <n_mels>
#     channels: [512, 512, 512, 1024]
#     kernel_sizes: [5, 3, 3, 1]
#     dilations: [1, 2, 3, 1]
#     attention_channels: 128
#     lin_neurons: !ref <emb_dim>
embedding_model: !new:models.pann.Cnn14
    mel_bins: !ref <n_mels>
    emb_dim: !ref <emb_dim>
    norm_type: !ref <emb_norm_type>

# classifier_fn: !name:speechbrain.lobes.models.ECAPA_TDNN.Classifier
#     input_size: !ref <emb_dim>
#     out_neurons: !ref <n_classes>

reinit_classifier: True
classifier_fn: !name:models.modules.Classifier
    input_size: !ref <emb_dim>
    output_size: !ref <n_classes>

modules:
    compute_features: !ref <compute_features>
    embedding_model: !ref <embedding_model>
    # classifier: !ref <classifier>
    mean_var_norm: !ref <mean_var_norm>

compute_cost: !new:losses.LogSoftmaxWithProbWrapper
    loss_fn: !new:torch.nn.Identity
    # loss_fn: !new:speechbrain.nnet.losses.AdditiveAngularMargin
    #     margin: 0.2
    #     scale: 30

acc_metric: !name:speechbrain.utils.Accuracy.AccuracyStats

# opt_class: !name:torch.optim.Adam
#     lr: !ref <base_lr>
#     weight_decay: 0.0005
#
# lr_scheduler_fn: !name:speechbrain.nnet.schedulers.CyclicLRScheduler
#     base_lr: !ref <final_lr>
#     max_lr: !ref <base_lr>
#     step_size: 888

opt_class: !name:torch.optim.SGD
    lr: !ref <base_lr>
    weight_decay: 0
    momentum: 0.9

lr_scheduler_fn: !name:schedulers.SimSiamCosineScheduler
    warmup_epochs: !ref <warmup_epochs>
    warmup_lr: !ref <warmup_lr>
    num_epochs: !ref <number_of_epochs>
    base_lr: !ref <base_lr>
    final_lr: !ref <final_lr>
    steps_per_epoch: 222

epoch_counter_fn: !name:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <number_of_epochs>

datapoint_counter: !new:utils.DatapointCounter
      
# # Logging + checkpoints
checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>

recoverables:
    # embedding_model: !ref <embedding_model>
    # classifier: !ref <classifier>
    # normalizer: !ref <mean_var_norm>
    datapoint_counter: !ref <datapoint_counter>

prev_checkpointer: null
# prev_checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
#     checkpoints_dir: !PLACEHOLDER

ssl_checkpoints_dir: "/mnt/data/zhepei/outputs/cssl_sound/tdnn_supsiam_rep100/2022-02-26+21-55-19_seed_1234+sup100_ssl000_fold1/save"

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <train_log>

# wandb
use_wandb: True
train_log_frequency: 20
wandb_logger_fn: !name:utils.MyWandBLogger
    initializer: !name:wandb.init
    entity: xi-j
    project: cssl_sound_BT_ann
    name: !ref <time_stamp>+seed_<seed>+<experiment_name>
    dir: !ref <output_folder>
    reinit: True
    yaml_config: hparams/tau19/linclf_train.yaml
    resume: False

